import numpy as np
import random

# Parameters
gamma = 0.9  # Discount factor
alpha = 0.1  # Learning rate
episodes = 10000  # Number of games to train
epsilon = 0.1  # Exploration rate

# Initialize the Q-Table (state-action pairs)
Q = {}
reward ={}

def get_state(board):
    """Convert the board to a string representation (for use as a Q-table key)."""
    return ''.join(map(str, board.flatten()))

def get_valid_actions(board):
    """Return a list of valid actions (empty positions)."""
    return [i for i in range(9) if board[i] == 0]

def choose_action(state, valid_actions):
    """Choose an action using epsilon-greedy strategy."""
    if random.uniform(0, 1) < epsilon:  # Explore
        return random.choice(valid_actions)
    else:  # Exploit
        if state not in Q:
            Q[state] = np.zeros(9)  # Initialize Q-values for new state
        return np.argmax([Q[state][a] if a in valid_actions else -np.inf for a in range(9)])

def check_winner(board):
    """Check if there's a winner or the game is a draw."""
    winning_combinations = [
        [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Rows
        [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Columns
        [0, 4, 8], [2, 4, 6]  # Diagonals
    ]
    for combo in winning_combinations:
        if board[combo[0]] == board[combo[1]] == board[combo[2]] != 0:
            return board[combo[0]]
    if 0 not in board:
        return 0  # Draw
    return None  # Game not over

# Training
for episode in range(episodes):
    board = np.zeros(9, dtype=int)  # Empty board
    current_player = 1  # Player 1 starts

    while True:
        state = get_state(board)
        valid_actions = get_valid_actions(board)
        action = choose_action(state, valid_actions)
        
        board[action] = current_player
        next_state = get_state(board)
        
        winner = check_winner(board)
        if winner is not None:
            reward = 1 if winner == 1 else -1 if winner == 2 else 0
            Q[state][action] = Q[state][action] + alpha * (reward - Q[state][action])
            break
        
        if state not in Q:
            Q[state] = np.zeros(9)
        if next_state not in Q:
            Q[next_state] = np.zeros(9)
        
        Q[state][action] = Q[state][action] + gamma * np.max(Q[next_state]) - Q[state][action]
        
        current_player = 3 - current_player  # Switch player

print(Q)
# Test the trained model to play optimally